{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dot Product Attention (Stateless)\n",
    "\n",
    "- **Code task:** Implement a basic function for dot product attention that takes queries, keys, and values as inputs and returns attention scores.\n",
    "\n",
    "### 2. Scaled Dot Product Attention\n",
    "\n",
    "- **Code task:** Modify the dot product attention function to include scaling by the square root of the key dimension for numerical stability.\n",
    "\n",
    "### 3. Self-Attention (Trainable)\n",
    "\n",
    "- **Code task:** Implement a trainable self-attention module that uses learnable weight matrices to transform queries, keys, and values.\n",
    "\n",
    "### 4. Causal Self-Attention (Includes Mask Param)\n",
    "\n",
    "- **Code task:** Extend the self-attention module to support causal masking, ensuring that information flow respects autoregressive constraints.\n",
    "\n",
    "### 5. Multi-Head Attention\n",
    "\n",
    "- **Code task:** Extend the causal self-attention module to support multiple heads, computes attention for each head, and combines the results.\n",
    "\n",
    "### 6. Feedforward Network\n",
    "\n",
    "- **Code task:** Implement a typical transformer feedforward network.\n",
    "\n",
    "### 7. Trainable Transformer Block\n",
    "\n",
    "- **Code task:** Construct a complete transformer block by combining the causal multi-head self-attention module with the feedforward network.\n",
    "\n",
    "### 8. Residual Connections\n",
    "\n",
    "- **Code task:** Incorporate residual connections into the transformer block to stabilize training and improve gradient flow.\n",
    "\n",
    "### 9. Layer Normalization\n",
    "\n",
    "- **Code task:** Add both layer normalization and RMS normalization to the transformer block, applying them in the appropriate sequence with residual connections. Ensure the implementations are modular with an initialization argument to select between the two (defaulting to RMS normalization), so either method can be easily selected and tested.\n",
    "\n",
    "### 10. Transformer Model (Decoder-Only)\n",
    "\n",
    "- **Code task:** Implement a decoder-only transformer model with the following components:\n",
    "  - An embedding layer to process input tokens.\n",
    "  - A learnable positional encoding layer.\n",
    "  - A stack of transformer blocks (as defined in previous steps).\n",
    "  - A final linear layer to project outputs to the vocabulary size for logits generation.\n",
    "\n",
    "### 11. Dropout Support\n",
    "\n",
    "- **Code task:** Add support for dropout in the following locations of the transformer model:\n",
    "  - Attention scores after the softmax operation in the attention mechanism.\n",
    "  - The output of the feedforward network in each transformer block.\n",
    "  - The output of the embedding layer, including positional encodings.\n",
    "  - The output of the stacked transformer blocks before the final linear layer.\n",
    "  - Ensure that dropout is configurable with a specified rate and is only applied during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def dot_product_attention(\n",
    "    queries: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> torch.Tensor:\n",
    "    q_len, q_dim = queries.shape[-2:]\n",
    "    k_len, k_dim = keys.shape[-2:]\n",
    "    v_len, v_dim = values.shape[-2:]\n",
    "    assert q_dim == k_dim, \"Queries and keys must have the same embedding size\"\n",
    "    assert k_len == v_len, \"Keys and values must be the same length\"\n",
    "    scores = queries @ keys.transpose(-2, -1)  # [...batch dimensions, q_len, k_len]\n",
    "    scores /= k_dim**0.5\n",
    "    if mask is not None:\n",
    "        assert mask.shape[-2:] == (\n",
    "            q_len,\n",
    "            k_len,\n",
    "        ), \"mask's last two dimensions must be equal to the query and key lengths respectively\"\n",
    "        scores += mask * -1e9\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    return weights @ values  # [...batch dimensions, q_len, v_dim]\n",
    "\n",
    "\n",
    "dot_product_attention(\n",
    "    queries=torch.randn([2, 3]),\n",
    "    keys=torch.randn([3, 3]),\n",
    "    values=torch.randn([3, 4]),\n",
    "    mask=torch.zeros([2, 3]),\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.wq = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wk = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wv = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert (\n",
    "            x.shape[-1] == self.dim\n",
    "        ), f\"Input x's last/innermost dimension {x.shape[-1]} does not match this self-attention module's embedding dim {self.dim}\"\n",
    "        return dot_product_attention(self.wq(x), self.wk(x), self.wv(x))\n",
    "\n",
    "\n",
    "attn = SelfAttention(4)\n",
    "attn.forward(torch.randn([2, 3, 4])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            dim % num_heads == 0\n",
    "        ), f\"The number of heads {num_heads} must evenly divide the embedding dimension {dim}\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.wq = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wk = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.wv = torch.nn.Linear(dim, dim, bias=False)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        *batch_dims, seq_len, dim = x.shape\n",
    "        assert (\n",
    "            dim == self.dim\n",
    "        ), f\"Input x's last/innermost dimension {dim} does not match this self-attention module's embedding dim {self.dim}\"\n",
    "        if mask is None:\n",
    "            mask = torch.triu(\n",
    "                torch.ones([seq_len, seq_len], dtype=x.dtype, device=x.device),\n",
    "                diagonal=1,\n",
    "            )\n",
    "        split = lambda x: x.view(\n",
    "            *batch_dims, seq_len, self.num_heads, self.head_dim\n",
    "        ).transpose(-3, -2)\n",
    "        return (\n",
    "            dot_product_attention(\n",
    "                queries=split(self.wq(x)),\n",
    "                keys=split(self.wk(x)),\n",
    "                values=split(self.wv(x)),\n",
    "                mask=mask,\n",
    "            )\n",
    "            .transpose(-3, -2)\n",
    "            .reshape(*batch_dims, seq_len, dim)\n",
    "        )\n",
    "\n",
    "\n",
    "attn = CausalSelfAttention(4, num_heads=2)\n",
    "attn.forward(torch.randn([2, 3, 4])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.w_in = torch.nn.Linear(dim, dim * 4)\n",
    "        self.w_out = torch.nn.Linear(dim * 4, dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.w_in(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return self.w_out(x)\n",
    "    \n",
    "ff = FeedForward(4)\n",
    "ff.forward(torch.randn(2, 3, 4)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
